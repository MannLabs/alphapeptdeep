{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---#| default_exp model.model_interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook mainly defines the basic interface that is used to interact with the deep learning models. Its 'public' functions are intended to stay untouched over the project, while the specific workings of the interface can be changed (i.e. programming polymorphism concept). For example, models can always be loaded with the `load()` function and details of the loading can be changed by inheriting the interface and changing the functions that `load()` calls. More details are given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peptdeep.model.model_interface import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = ModelInterface()\n",
    "m.fixed_sequence_len = 10\n",
    "df = pd.DataFrame({\n",
    "    'sequence':['ABCD']*5+['CDEFGHIJ']*5,\n",
    "    'mods': 'Oxidation@M;Oxidation@M',\n",
    "    'mod_sites': '2;2'\n",
    "})\n",
    "df['nAA'] = df.sequence.str.len()\n",
    "m._pad_zeros_if_fixed_len(df)\n",
    "assert m._get_features_from_batch_df(df).size()==(10,12)\n",
    "assert m._get_mod_features(df).size() == (10,12,109)\n",
    "assert (m._get_mod_features(df)[:,2,3]==2).all() # two Oxidation on one site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface Class\n",
    "The `ModelInterface` below is intended to provide a standardized way to handle deep learning models. It does not contain the PyTorch-based models themselves, but provides methods to `load()`, `save()`, `build()`, `train()` and `predict()` new models. These methods are intended to stay unchanged. \n",
    "To adapt the interface to a new usecase, we inherit the interface in a new class and re-implement the relevant method `_get_features_from_batch_df()`. Sometimes we also need to re-implement `_get_targets_from_batch_df()` and `_prepare_predict_data_df()`.\n",
    "\n",
    "The interface will adapt the training and prediction procedures. The implementation below will automatically empty the GPU cache at the end of `train()` and `predict()` to save GPU memory.\n",
    "\n",
    "For example, if we would like to design a new model for peptides with different purposes, for example RT prediction, we need to:\n",
    "\n",
    "- Design the pytorch model (`class RTPrediction(torch.nn.Module):...`).\n",
    "- Design the sub-class inherited from ModelInterface (`class RTPredictionModel(ModelInterface):...`).\n",
    "- In `__init__` method, define `self.target_column_to_train = \"detect_value\"` and `self.target_column_to_predict = \"predict_value\"`. Also define `self._min_pred_value = some_value`.\n",
    "- Re-implement `def _get_features_from_batch_df(self, batch_df): return self._get_aa_indice_features(batch_df)` (default) to predict property for sequence. For modified sequence, use `def _get_features_from_batch_df(self, batch_df): return self._get_aa_mod_features(batch_df)`.\n",
    "\n",
    "- At last, execute the model in a python script or a notebook:\n",
    "```\n",
    "model = RTPredictionModel()\n",
    "model.build(model_class=RTPrediction)\n",
    "df = ... # the training data\n",
    "model.train(df)\n",
    "pred_df = model.predict(df)\n",
    "```\n",
    "\n",
    "Check out `peptdeep.model.generic_property_prediction` for details. `peptdeep.model.rt.AlphaRTModel` and `peptdeep.model.ccs.AlphaCCSModel` are also similar. MS2 prediction model is more complicated as the output value for a peptide is not a scalar value, see `peptdeep.model.ms2.pDeepModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the APIs\n",
    "\n",
    "Building a model for peptide classification (e.g. detectability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, design the `torch.nn.Module` (Transformer model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import peptdeep.model.building_block as building_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Bert(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        nlayers = 3,\n",
    "        input_dim = 128, #ascii code number\n",
    "        hidden_dim = 256,\n",
    "        dropout = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Model based on a transformer Architecture from \n",
    "        Huggingface's BertEncoder class.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.input_nn =  torch.nn.Sequential(\n",
    "            torch.nn.Embedding(input_dim, hidden_dim),\n",
    "            building_block.PositionalEncoding(hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.hidden_nn = building_block.Hidden_HFace_Transformer(\n",
    "            hidden_dim, nlayers=nlayers, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.output_nn = torch.nn.Sequential(\n",
    "            building_block.SeqAttentionSum(hidden_dim),\n",
    "            torch.nn.PReLU(),\n",
    "            self.dropout,\n",
    "            torch.nn.Linear(hidden_dim, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.input_nn(x))\n",
    "\n",
    "        x = self.hidden_nn(x)\n",
    "        x = self.dropout(x[0])\n",
    "\n",
    "        return self.output_nn(x).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, implement the ModelInterface APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Model(ModelInterface):\n",
    "    def __init__(self, \n",
    "        dropout=0.1,\n",
    "        model_class:torch.nn.Module=Test_Bert, #model class defined above\n",
    "        device:str='gpu',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(device=device)\n",
    "        self.build(\n",
    "            model_class,\n",
    "            dropout=dropout,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.loss_func = torch.nn.BCELoss() # loss for binary classification\n",
    "        self.target_column_to_predict = 'predicted_prob'\n",
    "        self.target_column_to_train = 'detected_prob'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>detected_prob</th>\n",
       "      <th>nAA</th>\n",
       "      <th>predicted_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.876801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.876801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.876801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.876801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.876801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.860851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.860851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.860851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.860851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.860851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence  detected_prob  nAA  predicted_prob\n",
       "0      ABCD            1.0    4        0.876801\n",
       "1      ABCD            1.0    4        0.876801\n",
       "2      ABCD            1.0    4        0.876801\n",
       "3      ABCD            1.0    4        0.876801\n",
       "4      ABCD            1.0    4        0.876801\n",
       "5  CDEFGHIJ            1.0    8        0.860851\n",
       "6  CDEFGHIJ            1.0    8        0.860851\n",
       "7  CDEFGHIJ            1.0    8        0.860851\n",
       "8  CDEFGHIJ            1.0    8        0.860851\n",
       "9  CDEFGHIJ            1.0    8        0.860851"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'sequence':['ABCD']*5+['CDEFGHIJ']*5,\n",
    "})\n",
    "df['detected_prob'] = 1.0\n",
    "\n",
    "model = Test_Model()\n",
    "model.train(df, epoch=2)\n",
    "model.predict(df)\n",
    "assert 'predicted_prob' in df.columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>detected_prob</th>\n",
       "      <th>nAA</th>\n",
       "      <th>predicted_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.946883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.946883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.946883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.946883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABCD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.946883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.934433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.934433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.934433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.934433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CDEFGHIJ</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.934433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sequence  detected_prob  nAA  predicted_prob\n",
       "0      ABCD            1.0    4        0.946883\n",
       "1      ABCD            1.0    4        0.946883\n",
       "2      ABCD            1.0    4        0.946883\n",
       "3      ABCD            1.0    4        0.946883\n",
       "4      ABCD            1.0    4        0.946883\n",
       "5  CDEFGHIJ            1.0    8        0.934433\n",
       "6  CDEFGHIJ            1.0    8        0.934433\n",
       "7  CDEFGHIJ            1.0    8        0.934433\n",
       "8  CDEFGHIJ            1.0    8        0.934433\n",
       "9  CDEFGHIJ            1.0    8        0.934433"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training_groupby_nAA = False\n",
    "model.train(df, epoch=2)\n",
    "model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `build_from_py_codes()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peptdeep.model.ms2 import pDeepModel\n",
    "from peptdeep.pretrained_models import MODEL_ZIP_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (input_nn): Input_26AA_Mod_PositionalEncoding(\n",
       "    (mod_nn): Mod_Embedding_FixFirstK(\n",
       "      (nn): Linear(in_features=103, out_features=2, bias=False)\n",
       "    )\n",
       "    (aa_emb): Embedding(27, 240, padding_idx=0)\n",
       "    (pos_encoder): PositionalEncoding()\n",
       "  )\n",
       "  (meta_nn): Meta_Embedding(\n",
       "    (nn): Linear(in_features=9, out_features=7, bias=True)\n",
       "  )\n",
       "  (hidden_nn): Hidden_HFace_Transformer(\n",
       "    (bert): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_nn): Decoder_Linear(\n",
       "    (nn): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "      (2): Linear(in_features=64, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (modloss_nn): ModuleList(\n",
       "    (0): Hidden_HFace_Transformer(\n",
       "      (bert): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-08, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Decoder_Linear(\n",
       "      (nn): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (1): PReLU(num_parameters=1)\n",
       "        (2): Linear(in_features=64, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms2_model = pDeepModel()\n",
    "ms2_model.build_from_py_codes(\n",
    "    MODEL_ZIP_FILE_PATH, 'generic/ms2.pth.model.py', \n",
    "    include_model_params_yaml=True\n",
    ")\n",
    "\n",
    "ms2_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "\n",
    "from peptdeep.pretrained_models import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "model_mgr = ModelManager()\n",
    "model_mgr.ms2_model.set_device('mps')\n",
    "model_mgr.ms2_model.set_device('gpu')\n",
    "model_mgr.ms2_model.set_device('cuda')\n",
    "model_mgr.ms2_model.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
