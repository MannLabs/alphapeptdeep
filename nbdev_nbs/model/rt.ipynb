{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#automatically reload modules so that when changing some modules you do not need to explicitly reload or reset the ipython kernel\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retention Time Prediction Models\n",
    "\n",
    "This notebook implements deep learning models to predict the retention time of a given (modified) peptide. The components of the models are defined in the `building_block.ipynb` notebook. It is based on the interface defined in `model_interface.ipynb`, which defines standard interactions, such as loading data and training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from peptdeep.model.featurize import (\n",
    "    get_batch_aa_indices, \n",
    "    get_batch_mod_feature\n",
    ")\n",
    "\n",
    "from peptdeep.settings import model_const\n",
    "\n",
    "import peptdeep.model.model_interface as model_interface\n",
    "import peptdeep.model.building_block as building_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "mod_feature_size = len(model_const['mod_elements'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "irt peptides as reported by [Escher et al.]('https://doi.org/10.1002/pmic.201100463')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "IRT_PEPTIDE_DF = pd.DataFrame(\n",
    "    [['LGGNEQVTR', 'RT-pep a', -24.92, '', ''],\n",
    "    ['GAGSSEPVTGLDAK', 'RT-pep b', 0.00, '', ''],\n",
    "    ['VEATFGVDESNAK', 'RT-pep c', 12.39, '', ''],\n",
    "    ['YILAGVENSK', 'RT-pep d', 19.79, '', ''],\n",
    "    ['TPVISGGPYEYR', 'RT-pep e', 28.71, '', ''],\n",
    "    ['TPVITGAPYEYR', 'RT-pep f', 33.38, '', ''],\n",
    "    ['DGLDAASYYAPVR', 'RT-pep g', 42.26, '', ''],\n",
    "    ['ADVTPADFSEWSK', 'RT-pep h', 54.62, '', ''],\n",
    "    ['GTFIIDPGGVIR', 'RT-pep i', 70.52, '', ''],\n",
    "    ['GTFIIDPAAVIR', 'RT-pep k', 87.23, '', ''],\n",
    "    ['LFLQFGAQGSPFLK', 'RT-pep l', 100.00, '', '']],\n",
    "    columns=['sequence','pep_name','irt', 'mods', 'mod_sites']\n",
    ")\n",
    "IRT_PEPTIDE_DF['nAA'] = IRT_PEPTIDE_DF.sequence.str.len()\n",
    "\n",
    "\n",
    "#legacy\n",
    "irt_pep = IRT_PEPTIDE_DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions\n",
    "\n",
    "In the model definition classes, all layers of the model are defined, utilizing the \"building blocks\" defined in `building_blocks.ipynb`\n",
    "\n",
    "### Transformer based model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Model_RT_Bert(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        dropout = 0.1,\n",
    "        nlayers = 4,\n",
    "        hidden = 128,\n",
    "        output_attentions=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Model based on a transformer Architecture from \n",
    "        Huggingface's BertEncoder class.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.input_nn = building_block.AATransformerEncoding(hidden)\n",
    "\n",
    "        self._output_attentions = output_attentions\n",
    "        \n",
    "        self.hidden_nn = building_block.Hidden_HFace_Transformer(\n",
    "            hidden, nlayers=nlayers, dropout=dropout,\n",
    "            output_attentions=output_attentions\n",
    "        )\n",
    "\n",
    "        self.output_nn = torch.nn.Sequential(\n",
    "            building_block.SeqAttentionSum(hidden),\n",
    "            torch.nn.PReLU(),\n",
    "            self.dropout,\n",
    "            torch.nn.Linear(hidden, 1),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def output_attentions(self):\n",
    "        return self._output_attentions\n",
    "\n",
    "    @output_attentions.setter\n",
    "    def output_attentions(self, val:bool):\n",
    "        self._output_attentions = val\n",
    "        self.hidden_nn.output_attentions = val\n",
    "\n",
    "    def forward(self, \n",
    "        aa_indices, \n",
    "        mod_x,\n",
    "    ):\n",
    "        x = self.dropout(self.input_nn(\n",
    "            aa_indices, mod_x\n",
    "        ))\n",
    "\n",
    "        hidden_x = self.hidden_nn(x)\n",
    "        if self.output_attentions:\n",
    "            self.attentions = hidden_x[1]\n",
    "        else:\n",
    "            self.attentions = None\n",
    "        x = self.dropout(hidden_x[0]+x*0.2)\n",
    "\n",
    "        return self.output_nn(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Model_RT_LSTM_CNN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Model based on a combined CNN/LSTM architecture\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "        hidden = 256\n",
    "        self.rt_encoder = building_block.Encoder_26AA_Mod_CNN_LSTM_AttnSum(\n",
    "            hidden\n",
    "        )\n",
    "\n",
    "        self.rt_decoder = building_block.Decoder_Linear(\n",
    "            hidden,\n",
    "            1\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "        aa_indices, \n",
    "        mod_x,\n",
    "    ):\n",
    "        x = self.rt_encoder(aa_indices, mod_x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return self.rt_decoder(x).squeeze(1)\n",
    "#legacy\n",
    "Model_RT_LSTM = Model_RT_LSTM_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "The final model which is used to interact with the data is defined below, utilizing the `ModelInterface` class defined in `model_interface.ipynb` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AlphaRTModel(model_interface.ModelInterface):\n",
    "    def __init__(self, \n",
    "        dropout=0.1,\n",
    "        model_class:torch.nn.Module=Model_RT_LSTM_CNN, #model defined above\n",
    "        device:str='gpu',\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Class to predict retention times from precursor dataframes.\n",
    "        \"\"\"\n",
    "        super().__init__(device=device)\n",
    "        self.build(\n",
    "            model_class,\n",
    "            dropout=dropout,\n",
    "            **kwargs\n",
    "        )\n",
    "        self.target_column_to_predict = 'rt_pred'\n",
    "        self.target_column_to_train = 'rt_norm'\n",
    "\n",
    "    def _get_features_from_batch_df(self, \n",
    "        batch_df: pd.DataFrame,\n",
    "    ):\n",
    "        return (\n",
    "            self._get_26aa_indice_features(batch_df),\n",
    "            self._get_mod_features(batch_df)\n",
    "        )\n",
    "\n",
    "    def add_irt_column_to_precursor_df(self,\n",
    "        precursor_df: pd.DataFrame\n",
    "    ):\n",
    "        self.predict(IRT_PEPTIDE_DF)\n",
    "        # simple linear regression\n",
    "        rt_pred_mean = IRT_PEPTIDE_DF.rt_pred.mean()\n",
    "        irt_mean = IRT_PEPTIDE_DF.irt.mean()\n",
    "        x = IRT_PEPTIDE_DF.rt_pred.values - rt_pred_mean\n",
    "        y = IRT_PEPTIDE_DF.irt.values - irt_mean\n",
    "        slope = np.sum(x*y)/np.sum(x*x)\n",
    "        intercept = irt_mean - slope*rt_pred_mean\n",
    "        # end linear regression\n",
    "        precursor_df['irt_pred'] = precursor_df.rt_pred*slope + intercept\n",
    "        return precursor_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity checks on simple cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch=1, nAA=11, batch=1, loss=0.5976: 100%|██████████| 1/1 [00:00<00:00, 16.27it/s]\n",
      "Epoch=2, nAA=11, batch=1, loss=0.5916: 100%|██████████| 1/1 [00:00<00:00, 23.82it/s]\n",
      "Epoch=3, nAA=11, batch=1, loss=0.5859: 100%|██████████| 1/1 [00:00<00:00, 22.64it/s]\n",
      "Epoch=4, nAA=11, batch=1, loss=0.5779: 100%|██████████| 1/1 [00:00<00:00, 25.14it/s]\n",
      "Epoch=5, nAA=11, batch=1, loss=0.5764: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model_RT_LSTM_CNN(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (rt_encoder): Encoder_26AA_Mod_CNN_LSTM_AttnSum(\n",
      "    (mod_nn): Mod_Embedding_FixFirstK(\n",
      "      (nn): Linear(in_features=103, out_features=2, bias=False)\n",
      "    )\n",
      "    (input_cnn): SeqCNN(\n",
      "      (cnn_short): Conv1d(35, 35, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "      (cnn_medium): Conv1d(35, 35, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "      (cnn_long): Conv1d(35, 35, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "    )\n",
      "    (hidden_nn): SeqLSTM(\n",
      "      (rnn): LSTM(140, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (attn_sum): SeqAttentionSum(\n",
      "      (attn): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1, bias=False)\n",
      "        (1): Softmax(dim=1)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (rt_decoder): Decoder_Linear(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "      (1): PReLU(num_parameters=1)\n",
      "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "708224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "def initialize_model():\n",
    "\n",
    "    torch.manual_seed(1337)\n",
    "    model = AlphaRTModel()\n",
    "    model.device = torch.device('cpu')\n",
    "    model.model.to(model.device)\n",
    "    mod_hidden = len(model_const['mod_elements'])\n",
    "    model.model(torch.LongTensor([[1,2,3,4,5,6]]), torch.tensor([[[0.0]*mod_hidden]*6]))\n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "def create_test_dataframe_with_identical_rows(nrows = 10):\n",
    "    repeat = 10\n",
    "    precursor_df = pd.DataFrame({\n",
    "        'sequence': ['AGHCEWQMKYR']*nrows,\n",
    "        'mods': ['Acetyl@Protein N-term;Carbamidomethyl@C;Oxidation@M']*nrows,\n",
    "        'mod_sites': ['0;4;8']*nrows,\n",
    "        'nAA': [11]*nrows,\n",
    "        'rt_norm': [0.6]*nrows\n",
    "    })\n",
    "    return precursor_df\n",
    "\n",
    "repeat_row_df = create_test_dataframe_with_identical_rows()\n",
    "\n",
    "model.train(repeat_row_df, epoch=5, verbose_each_epoch=True)\n",
    "\n",
    "print(model.get_parameter_num())\n",
    "\n",
    "def test_prediction():\n",
    "    model = initialize_model()\n",
    "    repeat_row_df_predict = model.predict(repeat_row_df)\n",
    "    display(repeat_row_df_predict)\n",
    "    pred_rts = list(repeat_row_df_predict[\"rt_pred\"])\n",
    "    first_pred_rt = pred_rts[0]\n",
    "    control_array = [first_pred_rt for x in range(len(pred_rts))]\n",
    "    np.testing.assert_almost_equal(pred_rts, control_array)\n",
    "test_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from peptdeep.pretrained_models import ModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>pep_name</th>\n",
       "      <th>irt</th>\n",
       "      <th>mods</th>\n",
       "      <th>mod_sites</th>\n",
       "      <th>nAA</th>\n",
       "      <th>rt_pred</th>\n",
       "      <th>irt_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGGNEQVTR</td>\n",
       "      <td>RT-pep a</td>\n",
       "      <td>-24.92</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>0.072804</td>\n",
       "      <td>-28.148849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAGSSEPVTGLDAK</td>\n",
       "      <td>RT-pep b</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>0.271196</td>\n",
       "      <td>2.053492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VEATFGVDESNAK</td>\n",
       "      <td>RT-pep c</td>\n",
       "      <td>12.39</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0.332649</td>\n",
       "      <td>11.408902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YILAGVENSK</td>\n",
       "      <td>RT-pep d</td>\n",
       "      <td>19.79</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>0.400949</td>\n",
       "      <td>21.806524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TPVISGGPYEYR</td>\n",
       "      <td>RT-pep e</td>\n",
       "      <td>28.71</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.438901</td>\n",
       "      <td>27.584271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TPVITGAPYEYR</td>\n",
       "      <td>RT-pep f</td>\n",
       "      <td>33.38</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.489774</td>\n",
       "      <td>35.328937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DGLDAASYYAPVR</td>\n",
       "      <td>RT-pep g</td>\n",
       "      <td>42.26</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0.542729</td>\n",
       "      <td>43.390475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ADVTPADFSEWSK</td>\n",
       "      <td>RT-pep h</td>\n",
       "      <td>54.62</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0.609782</td>\n",
       "      <td>53.598396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GTFIIDPGGVIR</td>\n",
       "      <td>RT-pep i</td>\n",
       "      <td>70.52</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.757164</td>\n",
       "      <td>76.035216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GTFIIDPAAVIR</td>\n",
       "      <td>RT-pep k</td>\n",
       "      <td>87.23</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.846791</td>\n",
       "      <td>89.679588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LFLQFGAQGSPFLK</td>\n",
       "      <td>RT-pep l</td>\n",
       "      <td>100.00</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>0.857061</td>\n",
       "      <td>91.243048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sequence  pep_name     irt mods mod_sites  nAA   rt_pred   irt_pred\n",
       "0        LGGNEQVTR  RT-pep a  -24.92                   9  0.072804 -28.148849\n",
       "1   GAGSSEPVTGLDAK  RT-pep b    0.00                  14  0.271196   2.053492\n",
       "2    VEATFGVDESNAK  RT-pep c   12.39                  13  0.332649  11.408902\n",
       "3       YILAGVENSK  RT-pep d   19.79                  10  0.400949  21.806524\n",
       "4     TPVISGGPYEYR  RT-pep e   28.71                  12  0.438901  27.584271\n",
       "5     TPVITGAPYEYR  RT-pep f   33.38                  12  0.489774  35.328937\n",
       "6    DGLDAASYYAPVR  RT-pep g   42.26                  13  0.542729  43.390475\n",
       "7    ADVTPADFSEWSK  RT-pep h   54.62                  13  0.609782  53.598396\n",
       "8     GTFIIDPGGVIR  RT-pep i   70.52                  12  0.757164  76.035216\n",
       "9     GTFIIDPAAVIR  RT-pep k   87.23                  12  0.846791  89.679588\n",
       "10  LFLQFGAQGSPFLK  RT-pep l  100.00                  14  0.857061  91.243048"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "models = ModelManager(device='cpu')\n",
    "models.load_installed_models()\n",
    "pep_df = IRT_PEPTIDE_DF.iloc[:7].drop(columns=['irt'])\n",
    "df_w_rt_prediction = models.rt_model.predict(pep_df)\n",
    "df_w_irt_prediction_added = models.rt_model.add_irt_column_to_precursor_df(pep_df)\n",
    "\n",
    "assert df_w_irt_prediction_added.rt_pred.is_monotonic\n",
    "assert df_w_irt_prediction_added.irt_pred.is_monotonic\n",
    "df_w_rt_prediction = models.rt_model.predict(IRT_PEPTIDE_DF)\n",
    "models.rt_model.add_irt_column_to_precursor_df(IRT_PEPTIDE_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from peptdeep.utils import evaluate_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zengwenfeng/opt/anaconda3/lib/python3.8/site-packages/statsmodels/tsa/tsatools.py:130: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  x = pd.concat(x[::order], 1)\n",
      "/Users/zengwenfeng/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/stats.py:1541: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=11\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R_square</th>\n",
       "      <th>R</th>\n",
       "      <th>slope</th>\n",
       "      <th>intercept</th>\n",
       "      <th>test_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.994987</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.3828</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R_square         R   slope  intercept  test_num\n",
       "0      0.99  0.994987  0.9901     0.3828      11.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "evaluate_linear_regression(IRT_PEPTIDE_DF, 'irt', 'irt_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a3b27e141e49c996c9b863f8707e97aabd49c4a7e8445b9b783b34e4a21a9b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
