{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peptdeep.model.ms2 import pDeepModel, normalize_fragment_intensities\n",
    "from peptdeep.model.rt import IRT_PEPTIDE_DF\n",
    "from alphabase.spectral_library.flat import SpecLibFlat\n",
    "from alphabase.peptide.precursor import refine_precursor_df\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS2 Transfer Learning \n",
    "\n",
    "In this notebook we will walk you through multiple use cases for fine tuning the ms2 models. There various reasons why fine-tuning a pre-trained model can be a good option for example:\n",
    "- Improving performance on a new dataset while befitting from the pre-trained models provided by peptDeep. \n",
    "- Modifying the model prediction head to be able to predict additional fragment types. \n",
    "\n",
    "Here are the use cases we support in peptDeep: \n",
    "- Finetuning the same model architecture (with the same target charged fragment types) (1)\n",
    "- Finetuning a MS2 model for a new sharged fragment types. (2)\n",
    "\n",
    "(1)For the first use case we can benefit from the whole pretrained model, since no architecture changes is required we can directly finetune the model as long as we have a target value for all the fragment types the model is supporting. \n",
    "\n",
    "(2)For the second use case, the architecture has to change, since instead of predicting the default 8 fragment types we need to predict 12 fragament types for example. But instead of training the model from scratch, peptdeep allows changing the requested fragment types and still benefiting from a pre-trained model by only re-intializing a new prediction head. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def calculate_similarity(precursor_df_a, precursor_df_b, intensity_df_a, intensity_df_b):\n",
    "\n",
    "    _a_df = precursor_df_a[['precursor_idx', 'frag_start_idx', 'frag_stop_idx']].copy()\n",
    "    _b_df = precursor_df_b[['precursor_idx', 'frag_start_idx', 'frag_stop_idx']].copy()\n",
    "\n",
    "    _merged_df = pd.merge(_a_df, _b_df, on='precursor_idx', suffixes=('_a', '_b'))\n",
    "    # keep only first precursor\n",
    "    _merged_df = _merged_df.drop_duplicates(subset='precursor_idx', keep='first')\n",
    "    similarity_list = []\n",
    "\n",
    "    for i, (start_a, stop_a, start_b, stop_b) in enumerate(zip(_merged_df['frag_start_idx_a'], _merged_df['frag_stop_idx_a'], _merged_df['frag_start_idx_b'], _merged_df['frag_stop_idx_b'])):\n",
    "        observed_intensity = intensity_df_a.loc[start_a:stop_a, :].values.flatten()\n",
    "        predicted_intensity = intensity_df_b.loc[start_b:stop_b, :].values.flatten()\n",
    "\n",
    "        similarity = np.dot(observed_intensity, predicted_intensity) / ((np.linalg.norm(observed_intensity) * np.linalg.norm(predicted_intensity)) + 1e-6)\n",
    "        similarity_list.append({'similarity': similarity, 'index': i, 'precursor_idx': _merged_df.iloc[i]['precursor_idx']})\n",
    "\n",
    "    return pd.DataFrame(similarity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragment types in the training data: Index(['a_z1', 'a_z2', 'b_z1', 'b_z2', 'c_z1', 'c_z2', 'x_z1', 'x_z2', 'y_z1',\n",
      "       'y_z2', 'z_z1', 'z_z2', 'b_H2O_z1', 'b_H2O_z2', 'b_NH3_z1', 'b_NH3_z2',\n",
      "       'c_lossH_z1', 'c_lossH_z2', 'y_H2O_z1', 'y_H2O_z2', 'y_NH3_z1',\n",
      "       'y_NH3_z2', 'z_addH_z1', 'z_addH_z2', 'b_modloss_z1', 'b_modloss_z2',\n",
      "       'y_modloss_z1', 'y_modloss_z2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# loading test dataset for training the model\n",
    "trainin_data_path = \"../data/sample_speclib.hdf\"\n",
    "speclib = SpecLibFlat()\n",
    "speclib.load_hdf(trainin_data_path)\n",
    "speclib.fragment_intensity_df[\"b_modloss_z1\"] = 0\n",
    "speclib.fragment_intensity_df[\"b_modloss_z2\"] = 0\n",
    "speclib.fragment_intensity_df[\"y_modloss_z1\"] = 0\n",
    "speclib.fragment_intensity_df[\"y_modloss_z2\"] = 0\n",
    "fragment_types_in_data = speclib.fragment_intensity_df.columns\n",
    "\n",
    "speclib.precursor_df['nce'] = 30.0\n",
    "speclib.precursor_df['instrument'] = \"Lumos\"\n",
    "speclib.precursor_df['precursor_idx'] = speclib.precursor_df.index  \n",
    "\n",
    "# sample only 100 samples\n",
    "# speclib.precursor_df = speclib.precursor_df.sample(100)\n",
    "refine_precursor_df(speclib.precursor_df)\n",
    "# normalize intensity \n",
    "normalize_fragment_intensities(speclib.precursor_df, speclib.fragment_intensity_df)\n",
    "print(f\"Fragment types in the training data: {fragment_types_in_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../pretrained_models/generic/ms2.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training a model to enhance performance without changing the supported fragment types for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragment types supported by the model: ['b_z1', 'b_z2', 'y_z1', 'y_z2', 'b_modloss_z1', 'b_modloss_z2', 'y_modloss_z1', 'y_modloss_z2']\n"
     ]
    }
   ],
   "source": [
    "model_interface = pDeepModel(override_from_weights=True)\n",
    "model_interface.load(model_path)\n",
    "\n",
    "supported_fragment_types = model_interface.model.supported_charged_frag_types\n",
    "print(f\"Fragment types supported by the model: {supported_fragment_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-24 06:27:55> Training with fixed sequence length: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Epoch=1, Mean Loss=0.01714216558262706\n",
      "[Training] Epoch=2, Mean Loss=0.014999589333310723\n",
      "[Training] Epoch=3, Mean Loss=0.01401507736183703\n",
      "[Training] Epoch=4, Mean Loss=0.013427118873223662\n",
      "[Training] Epoch=5, Mean Loss=0.01310566863976419\n",
      "[Training] Epoch=6, Mean Loss=0.012583116488531231\n",
      "[Training] Epoch=7, Mean Loss=0.01228506090119481\n",
      "[Training] Epoch=8, Mean Loss=0.011949550099670888\n",
      "[Training] Epoch=9, Mean Loss=0.011700581423938275\n",
      "[Training] Epoch=10, Mean Loss=0.011566510824486613\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model_interface.train(precursor_df=speclib.precursor_df, fragment_intensity_df=speclib.fragment_intensity_df.loc[:,supported_fragment_types], epoch=10, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training on fragment types: ['b_z1', 'b_z2', 'y_z1', 'y_z2', 'b_modloss_z1', 'b_modloss_z2', 'y_modloss_z1', 'y_modloss_z2']\n",
      "Median similarity: 0.9788682196185345\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction after training on fragment types: {supported_fragment_types}\") \n",
    "prediction_precursor_df = speclib.precursor_df.copy()\n",
    "prediction_precursor_df.drop(columns=[\"frag_start_idx\", \"frag_stop_idx\"], inplace=True)\n",
    "predictions = model_interface.predict(prediction_precursor_df) # predict wil set the frag_start_idx and frag_stop_idx inplace\n",
    "similarity_df = calculate_similarity(speclib.precursor_df, prediction_precursor_df, speclib.fragment_intensity_df.loc[:, supported_fragment_types], predictions)\n",
    "print(f\"Median similarity: {similarity_df['similarity'].median()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the interesting use case, where we can benefit from a pre-trained model with different charged fragment types compared to what we actually want to train on (12 Fragment types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before seeing why this might be a good option. Lets try training a model from scratch instead of fine tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested fragment types: ['b_z1', 'b_z2', 'y_z1', 'y_z2', 'b_H2O_z1', 'b_H2O_z2', 'b_NH3_z1', 'b_NH3_z2', 'y_H2O_z1', 'y_H2O_z2', 'y_NH3_z1', 'y_NH3_z2']\n",
      "2025-03-24 06:36:08> Training with fixed sequence length: 0\n",
      "[Training] Epoch=1, Mean Loss=0.18290741086006165\n",
      "[Training] Epoch=2, Mean Loss=0.12341278985142708\n",
      "[Training] Epoch=3, Mean Loss=0.09590978175401688\n",
      "[Training] Epoch=4, Mean Loss=0.07777980178594589\n",
      "[Training] Epoch=5, Mean Loss=0.06758781857788562\n",
      "[Training] Epoch=6, Mean Loss=0.06245427638292313\n",
      "[Training] Epoch=7, Mean Loss=0.05913622558116913\n",
      "[Training] Epoch=8, Mean Loss=0.05696883402764797\n",
      "[Training] Epoch=9, Mean Loss=0.0552051093429327\n",
      "[Training] Epoch=10, Mean Loss=0.05408928550779819\n",
      "[Training] Epoch=11, Mean Loss=0.05306584112346172\n",
      "[Training] Epoch=12, Mean Loss=0.05200395107269287\n",
      "[Training] Epoch=13, Mean Loss=0.051182358488440514\n",
      "[Training] Epoch=14, Mean Loss=0.04973145142197609\n",
      "[Training] Epoch=15, Mean Loss=0.04895635634660721\n",
      "[Training] Epoch=16, Mean Loss=0.048286024779081345\n",
      "[Training] Epoch=17, Mean Loss=0.04753392614424229\n",
      "[Training] Epoch=18, Mean Loss=0.04690856710076332\n",
      "[Training] Epoch=19, Mean Loss=0.04638163231313228\n",
      "[Training] Epoch=20, Mean Loss=0.0458812952786684\n"
     ]
    }
   ],
   "source": [
    "requested_fragment_types = ['b_z1', 'b_z2', 'y_z1', 'y_z2', 'b_H2O_z1','b_H2O_z2','b_NH3_z1', 'b_NH3_z2', 'y_H2O_z1', 'y_H2O_z2','y_NH3_z1', 'y_NH3_z2']\n",
    "print(f\"Requested fragment types: {requested_fragment_types}\")\n",
    "\n",
    "model_interface = pDeepModel(charged_frag_types=requested_fragment_types) # creating a model from scratch\n",
    "model_interface.train(precursor_df=speclib.precursor_df, fragment_intensity_df=speclib.fragment_intensity_df.loc[:,requested_fragment_types], epoch=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training on fragment types: ['b_z1', 'b_z2', 'y_z1', 'y_z2', 'b_H2O_z1', 'b_H2O_z2', 'b_NH3_z1', 'b_NH3_z2', 'y_H2O_z1', 'y_H2O_z2', 'y_NH3_z1', 'y_NH3_z2']\n",
      "Median similarity: 0.6148259002143182\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction after training on fragment types: {requested_fragment_types}\") \n",
    "prediction_precursor_df = speclib.precursor_df.copy()\n",
    "prediction_precursor_df.drop(columns=[\"frag_start_idx\", \"frag_stop_idx\"], inplace=True)\n",
    "predictions = model_interface.predict(prediction_precursor_df) # predict wil set the frag_start_idx and frag_stop_idx inplace\n",
    "similarity_df = calculate_similarity(speclib.precursor_df, prediction_precursor_df, speclib.fragment_intensity_df.loc[:,requested_fragment_types], predictions)\n",
    "print(f\"Median similarity: {similarity_df['similarity'].median()}\"  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see for the same requested fragment types, but this time we will use a pre-trained model as base for the fine-tuning ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested fragment types: ['b_z1', 'b_z2', 'y_z1', 'y_z2', 'b_H2O_z1', 'b_H2O_z2', 'b_NH3_z1', 'b_NH3_z2', 'y_H2O_z1', 'y_H2O_z2', 'y_NH3_z1', 'y_NH3_z2']\n",
      "2025-03-24 06:37:51> Training with fixed sequence length: 0\n",
      "[Training] Epoch=1, Mean Loss=0.13062387451529503\n",
      "[Training] Epoch=2, Mean Loss=0.11805687174201011\n",
      "[Training] Epoch=3, Mean Loss=0.1085664016008377\n",
      "[Training] Epoch=4, Mean Loss=0.0950324584543705\n",
      "[Training] Epoch=5, Mean Loss=0.0680735632032156\n",
      "[Training] Epoch=6, Mean Loss=0.05339862667024135\n",
      "[Training] Epoch=7, Mean Loss=0.044806683547794816\n",
      "[Training] Epoch=8, Mean Loss=0.04099671371281147\n",
      "[Training] Epoch=9, Mean Loss=0.038800872154533865\n",
      "[Training] Epoch=10, Mean Loss=0.03747128788381815\n",
      "[Training] Epoch=11, Mean Loss=0.03655099641531706\n",
      "[Training] Epoch=12, Mean Loss=0.03583796564489603\n",
      "[Training] Epoch=13, Mean Loss=0.035318268463015556\n",
      "[Training] Epoch=14, Mean Loss=0.034866768941283224\n",
      "[Training] Epoch=15, Mean Loss=0.03474729720503092\n",
      "[Training] Epoch=16, Mean Loss=0.03452551331371069\n",
      "[Training] Epoch=17, Mean Loss=0.03433547958731651\n",
      "[Training] Epoch=18, Mean Loss=0.034083543345332146\n",
      "[Training] Epoch=19, Mean Loss=0.03392626084387303\n",
      "[Training] Epoch=20, Mean Loss=0.0337152773886919\n"
     ]
    }
   ],
   "source": [
    "print(f\"Requested fragment types: {requested_fragment_types}\")\n",
    "model_interface = pDeepModel(charged_frag_types=requested_fragment_types)\n",
    "model_interface.load(model_path)\n",
    "model_interface.train(precursor_df=speclib.precursor_df, fragment_intensity_df=speclib.fragment_intensity_df.loc[:,requested_fragment_types], epoch=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction after training on fragment types: ['b_z1', 'b_z2', 'y_z1', 'y_z2', 'b_H2O_z1', 'b_H2O_z2', 'b_NH3_z1', 'b_NH3_z2', 'y_H2O_z1', 'y_H2O_z2', 'y_NH3_z1', 'y_NH3_z2']\n",
      "Median similarity: 0.7050544920322075\n"
     ]
    }
   ],
   "source": [
    "print(f\"Prediction after training on fragment types: {requested_fragment_types}\") \n",
    "prediction_precursor_df = speclib.precursor_df.copy()\n",
    "prediction_precursor_df.drop(columns=[\"frag_start_idx\", \"frag_stop_idx\"], inplace=True)\n",
    "predictions = model_interface.predict(prediction_precursor_df) # predict wil set the frag_start_idx and frag_stop_idx inplace\n",
    "similarity_df = calculate_similarity(speclib.precursor_df, prediction_precursor_df, speclib.fragment_intensity_df.loc[:,requested_fragment_types], predictions)\n",
    "print(f\"Median similarity: {similarity_df['similarity'].median()}\"  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peptdeep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
